services:
  # =============================================================
  # CADDY - Reverse Proxy (HTTPS + Security Headers)
  # =============================================================
  caddy:
    image: caddy:2-alpine
    container_name: dark-gpt-caddy
    ports:
      - "127.0.0.1:443:443"
      - "127.0.0.1:80:80"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - ./certs:/etc/caddy/certs:ro
      - caddy_data:/data
      - caddy_logs:/var/log/caddy
    depends_on:
      webui:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - dark-gpt-network

  # =============================================================
  # OPEN-WEBUI - Chat Interface (with Authentication)
  # =============================================================
  webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: dark-gpt-webui
    # Port NOT exposed externally - only accessible via Caddy
    expose:
      - "8080"
    volumes:
      - webui_data:/app/backend/data
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      - WEBUI_AUTH=${WEBUI_AUTH:-true}
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
      - DEFAULT_MODELS=${DEFAULT_MODELS:-dolphin-llama3:8b}
      - ENABLE_SIGNUP=${ENABLE_SIGNUP:-true}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - dark-gpt-network

  # =============================================================
  # OPTION: Mode Isolé (Tests automatisés, sécurité maximale)
  # Décommenter pour utiliser Ollama en isolation totale
  # =============================================================
  # ollama-isolated:
  #   image: ollama/ollama:latest
  #   container_name: dark-gpt-ollama-isolated
  #   network_mode: "none"  # ISOLATION TOTALE
  #   volumes:
  #     - ollama_data:/root/.ollama
  #     - ~/.ollama:/root/.ollama:ro
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   restart: unless-stopped

volumes:
  webui_data:
  caddy_data:
  caddy_logs:
  # ollama_data:

networks:
  dark-gpt-network:
    driver: bridge

# =============================================================
# USAGE
# =============================================================
#
# 1. Setup HTTPS (first time):
#    ./scripts/setup-https.sh
#
# 2. Start services:
#    docker compose up -d
#
# 3. Access (HTTPS):
#    https://dark-gpt.local
#
# 4. First user to register becomes admin
#
# 5. Stop:
#    docker compose down
#
# =============================================================
# SECURITY FEATURES
# =============================================================
#
# ✅ HTTPS via Caddy with mkcert certificates
# ✅ Authentication enabled (WEBUI_AUTH=true)
# ✅ Port 443/80 bound to localhost only
# ✅ WebUI not directly exposed (only via Caddy)
# ✅ Security headers (XSS, HSTS, etc.)
# ✅ Health checks
# ✅ Separate Docker network
#
