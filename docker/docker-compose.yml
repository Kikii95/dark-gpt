version: '3.8'

services:
  # =============================================================
  # OPTION 1 : Mode Isolé (Tests automatisés, sécurité maximale)
  # Décommenter pour utiliser Ollama en isolation totale
  # =============================================================
  # ollama-isolated:
  #   image: ollama/ollama:latest
  #   container_name: dark-gpt-ollama-isolated
  #   network_mode: "none"  # ISOLATION TOTALE
  #   volumes:
  #     - ollama_data:/root/.ollama
  #     - ~/.ollama:/root/.ollama:ro
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   restart: unless-stopped

  # =============================================================
  # OPTION 2 : Open-WebUI (Interface web, connecté à Ollama host)
  # =============================================================
  webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: dark-gpt-webui
    ports:
      - "127.0.0.1:3002:8080"  # Localhost only (3000 utilisé par autre projet)
    volumes:
      - webui_data:/app/backend/data
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - WEBUI_AUTH=false  # Pas d'auth (usage local)
      - DEFAULT_MODELS=dolphin-llama3:8b
      - ENABLE_SIGNUP=false
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  ollama_data:
  webui_data:

# =============================================================
# USAGE
# =============================================================
#
# Démarrer Open-WebUI (nécessite Ollama host running):
#   docker compose up -d
#
# Accès:
#   http://localhost:3000
#
# Arrêter:
#   docker compose down
#
# Logs:
#   docker compose logs -f webui
#
# =============================================================
# SÉCURITÉ
# =============================================================
#
# ✅ Port 3000 bindé sur 127.0.0.1 (localhost only)
# ✅ Données persistantes dans volume Docker
# ✅ Pas de telemetry (100% self-hosted)
# ✅ Auth désactivée (usage local personnel)
# ✅ Ollama sur host = pas de GPU dans container
